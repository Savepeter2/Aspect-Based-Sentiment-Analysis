# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A-uSmejMZkeuA397BMYccX48BN3jH5IK
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import torch
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stops=stopwords.words('english')

test_data = pd.read_csv('/content/drive/MyDrive/test - test.csv')

test_texts = test_data.text.values.tolist()

test_texts[:2]

test_data

class SentimentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
!pip install transformers
import torch
from torch.utils.data import Dataset
from transformers import DistilBertTokenizerFast,DistilBertForSequenceClassification
from transformers import Trainer,TrainingArguments

model = DistilBertForSequenceClassification.from_pretrained("/content/drive/MyDrive/ABSA", local_files_only = True)
trainer = Trainer(model=model)
trainer.model = model.cuda()



tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased',
                                                    num_labels=2)

test_data['label'] = 0 
test_texts = test_data['text'].values.tolist() 
test_labels = test_data['label'].values.tolist() 
test_encodings = tokenizer(test_texts, truncation=True, padding=True,return_tensors = 'pt') 
test_dataset = SentimentDataset(test_encodings, test_labels)


preds = trainer.predict(test_dataset=test_dataset)

probs = torch.from_numpy(preds[0]).softmax(1)

predictions = probs.numpy()# convert tensors to numpy array
new_df = pd.DataFrame(predictions,columns=['Positive_1','Negative_0'])
new_df.head()

def labels(x):
  if x == 0:
    return 'Negative_0'
  else:
    return 'Positive_1'

results = np.argmax(predictions,axis=1)
test_data['label'] = results
test_data['label'] = test_data['label'].map(labels)
test_data.head(10)

test_data.to_csv('results.csv', index = False)

sns.countplot(test_data.label, data = test_data)

#We can infer from the model that there are more negative reviews than positive reviews

#So now we'll test the trainer on some few examples and view the predictions
import numpy as np
import pandas as pd
examp = ['cannot rely on both milk delivery and grocery', 'your customer service is terrible!', 'i love notion as a tool', 'notion is a great site and an Iphone app', 'Asked for a workspace name or billing email address']
examp = np.array(examp)

examp = pd.DataFrame(examp, columns = ['text'])

examp

examp['label'] = 0 
examp_texts = examp['text'].values.tolist() 
examp_labels = examp['label'].values.tolist() 
examp_encodings = tokenizer(examp_texts, truncation=True, padding=True,return_tensors = 'pt') 
examp_dataset = SentimentDataset(examp_encodings, examp_labels)


preds_examp = trainer.predict(test_dataset=examp_dataset)
probs_examp = torch.from_numpy(preds_examp[0]).softmax(1)

examp_predictions = probs_examp.numpy()# convert tensors to numpy array
examp_df = pd.DataFrame(examp_predictions,columns=['Positive_1','Negative_0'])
examp_df.head()

def labels(x):
  if x == 0:
    return 'Negative_0'
  else:
    return 'Positive_1'

results = np.argmax(examp_predictions,axis=1)
examp['label'] = results
examp['label'] = examp['label'].map(labels)
examp.head(5)

#We can also see it also performed well on these few examples by generating right predictions.

